Slide 1: Cover Slide
- Title: Open-Weight LLM Landscape and Architecture Comparison
- Subtitle: Exploring Key Models, Architectures, and Trends
Visual/Images: https://sebastianraschka.com/images/blog/2025/the-big-llm-architecture-comparison/hero.jpg

Slide 2: Table of Contents
- Open-Weight LLMs Overview
- Transformer Basics
- Main Open Models
- Dense vs. MoE
- Positional Embeddings
- Attention Improvements
- Context Length & Performance
- Future Trends
Visual/Images: https://sebastianraschka.com/images/blog/2025/the-big-llm-architecture-comparison/18.png

Slide 3: Open-Weight LLMs Overview
- Definition: Trained weights publicly released for full access to architecture, code, weights (unlike closed models)
- Importance: Transparency, community improvements, cost-effective customization, rapid innovation; rivals closed models in benchmarks
Visual/Images: https://sebastianraschka.com/images/blog/2025/the-big-llm-architecture-comparison/hero.jpg

Slide 4: Basic Transformer Architecture
- Stacked decoder layers: Self-attention, FFN MLP, RMSNorm/LayerNorm, residual connections
- Key: Multi-head causal self-attention + token embeddings + positional info
Visual/Images: https://sebastianraschka.com/images/blog/2025/the-big-llm-architecture-comparison/11.png

Slide 5: Main Open Models
- LLaMA (Meta): Dense (e.g., Llama 3.1 405B), RoPE, GQA; benchmark leader
- Mistral: Dense (e.g., Nemo), sliding window attention
- Mixtral (Mistral AI): MoE (e.g., 8x22B), subset expert activation
Visual/Images: https://sebastianraschka.com/images/blog/2025/the-big-llm-architecture-comparison/18.png

Slide 6: Dense vs. MoE
- Dense: All params active (e.g., LLaMA 3.1, Gemma 2); high compute, consistent perf
- MoE: Sparseâ€”routes to expert subset (e.g., Mixtral 8x22B: 141B total, 30B active); efficient inference
Visual/Images: https://sebastianraschka.com/images/blog/2025/the-big-llm-architecture-comparison/9.png

Slide 7: Positional Embeddings
- RoPE (LLaMA/Mistral): Relative positioning for long contexts
- Alternatives: Alibi, absolute sinusoidal
- Encodes token order without fixed limits
Visual/Images: https://sebastianraschka.com/images/blog/2025/the-big-llm-architecture-comparison/11.png

Slide 8: Attention Improvements
- GQA: Reduces KV cache (LLaMA 3.1)
- Sliding Window: Local efficiency (Gemma 3, Mistral)
- MLA: Memory savings (DeepSeek V3); others: Linear/Sparse (Qwen)
Visual/Images: https://sebastianraschka.com/images/blog/2025/the-big-llm-architecture-comparison/18.png

Slide 9: Context Length, Performance & Future Trends
- Context: 128K+ (LLaMA 3.1, Mistral Large 2); improves reasoning, mitigated by windows
- Perf: Open models (e.g., DeepSeek-V3) lead efficiency/benchmarks
- Trends: More MoE/MLA, longer contexts, multimodal, reasoning gains
Visual/Images: https://sebastianraschka.com/images/blog/2025/the-big-llm-architecture-comparison/hero.jpg

Slide 10: Thank You
- Questions?
- Contact info or references
Visual/Images: https://devopedia.org/images/article/483/6445.1717130942.jpg