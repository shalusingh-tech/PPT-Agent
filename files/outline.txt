Slide 1: Cover Slide
- Open-Weight LLM Landscape and Architecture Comparison
Visual/Images: https://media.licdn.com/dms/image/sync/v2/D4D27AQHQauAMzq5wgA/articleshare-shrink_800/B4DZkzq1i_JQAI-/0/1757508491818?e=2147483647&v=beta&t=4EDC39BxEgVNAouFm1YDy5mhjlvBIyDKNstilWUO9Co

Slide 2: Table of Contents
- What are Open-Weight LLMs?
- Why They Are Important
- Basic Transformer Architecture
- Main Open Models (LLaMA, Mistral, Mixtral)
- Dense vs. MoE
- Positional Embeddings
- Attention Improvements
- Context Length & Performance
- Future Trends
Visual/Images: https://sebastianraschka.com/images/blog/2025/the-big-llm-architecture-comparison/11.png

Slide 3: What are Open-Weight LLMs?
- Modern LLMs from transformer blocks
- Components: self-attention, MLP (feedforward), normalization, learned weights
- Share code, training data, ablation studies (e.g., OLMo 2 for transparency)
Visual/Images: https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https://substack-post-media.s3.amazonaws.com/public/images/53956ae8-9cd8-474e-8c10-ef6bddb88164_1600x938.png

Slide 4: Why They Are Important
- Transparency enables research community access (code, data, ablations)
- Innovations reduce memory for long contexts
- Enable tasks: book summarization, legal docs, code analysis
- Democratize AI beyond closed models
Visual/Images: https://sebastianraschka.com/images/blog/2025/the-big-llm-architecture-comparison/11.png

Slide 5: Basic Transformer Architecture
- Layered: self-attention, feedforward (MLP) layers, normalization
- Core for context handling in modern LLMs
Visual/Images: https://sebastianraschka.com/images/blog/2025/the-big-llm-architecture-comparison/18.png

Slide 6: Main Open Models (LLaMA, Mistral, Mixtral)
- Leaders: LLaMA series, Mistral, Mixtral
- Compared on architecture, efficiency, performance
Visual/Images: https://media.licdn.com/dms/image/sync/v2/D4D27AQHQauAMzq5wgA/articleshare-shrink_800/B4DZkzq1i_JQAI-/0/1757508491818?e=2147483647&v=beta&t=4EDC39BxEgVNAouFm1YDy5mhjlvBIyDKNstilWUO9Co

Slide 7: Dense vs. MoE
- Dense: activates all parameters
- MoE (Mixture-of-Experts): activates parameter subsets (e.g., Mixtral)
- MoE cuts compute, maintains performance
Visual/Images: https://www.maginative.com/content/images/2023/12/GBEQckjX0AAoGq-.jpg

Slide 8: Positional Embeddings
- Encode sequence order in transformers
- Key for context/attention processing
Visual/Images: https://sebastianraschka.com/images/blog/2025/the-big-llm-architecture-comparison/18.png

Slide 9: Attention Improvements
- GQA (Grouped Query Attention)
- Sliding Windows
- Variants: causal/self-attention
- Reduce memory for long contexts
Visual/Images: https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https://substack-post-media.s3.amazonaws.com/public/images/53956ae8-9cd8-474e-8c10-ef6bddb88164_1600x938.png

Slide 10: Context Length & Performance
- Sliding windows enable huge contexts (books, legal docs, code)
- Memory/performance trade-offs
- Boosted by attention innovations
Visual/Images: https://sebastianraschka.com/images/blog/2025/the-big-llm-architecture-comparison/11.png

Slide 11: Future Trends
- More efficiency, transparency (e.g., OLMo 2)
- Advanced: MoE dominance, long-context handling
- Open-weight research directions
Visual/Images: https://media.licdn.com/dms/image/sync/v2/D4D27AQHQauAMzq5wgA/articleshare-shrink_800/B4DZkzq1i_JQAI-/0/1757508491818?e=2147483647&v=beta&t=4EDC39BxEgVNAouFm1YDy5mhjlvBIyDKNstilWUO9Co

Slide 12: Thank You
- Questions?
Visual/Images: 