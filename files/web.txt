[{'url': 'https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison', 'title': 'The Big LLM Architecture Comparison - Ahead of AI', 'content': '[![Image 1: Ahead of AI](https://substackcdn.com/image/fetch/$s_!96vs!,w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png)](https://magazine.sebastianraschka.com/) [![Image 2: Ahead of AI](https://substackcdn.com/image/fetch/$s_!xQ0c!,e_trim:10:white/e_trim:10:transparent/h_108,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5083e6d3-fbc9-4870-95b9-6e85d02f62a6_9366x2023.png)](https://magazine.sebastianraschka.com/) [![Image 6: Understanding Reasoning LLMs](https://substackcdn.com/image/fetch/$s_!QwUc!,w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png) #### Understanding Reasoning LLMs [Sebastian Raschka, PhD](https://substack.com/profile/27393275-sebastian-raschka-phd) · Feb 5 [Read full story](https://magazine.sebastianraschka.com/p/understanding-reasoning-llms)](https://magazine.sebastianraschka.com/p/understanding-reasoning-llms) [![Image 32: From GPT-2 to gpt-oss: Analyzing the Architectural Advances](https://substackcdn.com/image/fetch/$s_!kftt!,w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F529c4cc7-161c-4d7c-b186-06e68c771776_1564x926.png) #### From GPT-2 to gpt-oss: Analyzing the Architectural Advances [Sebastian Raschka, PhD](https://substack.com/profile/27393275-sebastian-raschka-phd) · Aug 9 [Read full story](https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the)](https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the) [![Image 39: From GPT-2 to gpt-oss: Analyzing the Architectural Advances](https://substackcdn.com/image/fetch/$s_!kftt!,w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F529c4cc7-161c-4d7c-b186-06e68c771776_1564x926.png) #### From GPT-2 to gpt-oss: Analyzing the Architectural Advances [Sebastian Raschka, PhD](https://substack.com/profile/27393275-sebastian-raschka-phd) · Aug 9 [Read full story](https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the)](https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the) [![Image 48: Understanding and Coding Self-Attention, Multi-Head Attention, Causal-Attention, and Cross-Attention in LLMs](https://substackcdn.com/image/fetch/$s_!3NS4!,w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69bfee26-ea3b-42a6-8a1a-6b8187852082_738x564.png) #### Understanding and Coding Self-Attention, Multi-Head Attention, Causal-Attention, and Cross-Attention in LLMs January 14, 2024 [Read full story](https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention)](https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention) [![Image 56: Build a Large Language Model (From Scratch)](https://substackcdn.com/image/fetch/$s_!RCl_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27a118a0-5da6-4486-b1f0-3743754d0a77_8106x4044.webp)](https://substackcdn.com/image/fetch/$s_!RCl_!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27a118a0-5da6-4486-b1f0-3743754d0a77_8106x4044.webp)', 'score': 0.568632, 'raw_content': None}, {'url': 'https://www.linkedin.com/posts/progressivethinker_sebastian-raschka-phd-continues-to-be-one-activity-7396505220354322433-P2rC', 'title': "Sebastian Raschka's 2025 LLM Landscape: A Comprehensive Guide", 'content': 'In the meantime, here is the architectural cheat sheet of the 17 models he analyzed: ➡️\xa0DeepSeek V3 & R1 →\xa0MLA (Multi-Head Latent Attention). ➡️\xa0DeepSeek V3.2 →\xa0Sparse Attention. ➡️\xa0Gemma 3 →\xa0Sliding Window Attention. ➡️\xa0MiniMax-M1 →\xa0Lightning Attention. →\xa0Cache: DeepSeek (MLA) →\xa0Window: Gemma (Sliding) →\xa0Speed: Qwen/Kimi (Linear Attention) (I’ll put the blog link in the comments.) ♻️ Repost to save someone $$$ and a lot of confusion. Pallavi Ahuja   1w  Fantastic breakdown, Pallavi Ahuja Sebastian’s insights are a must-read for anyone navigating the evolving LLM landscape. By clicking Continue to join or sign in, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy.', 'score': 0.47154015, 'raw_content': None}]